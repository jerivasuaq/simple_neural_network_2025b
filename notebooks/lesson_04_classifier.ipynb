{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d11a774",
   "metadata": {},
   "source": [
    "\n",
    "# MNIST Softmax Classifier — Forward/Backward Pass (Step‑by‑Step) + Animations\n",
    "\n",
    "You’ll build and **understand** a single‑layer neural network (softmax classifier / multinomial logistic regression) for MNIST **from scratch** in NumPy.  \n",
    "We’ll cover:\n",
    "1. Data prep (flattening 28×28 → 784).\n",
    "2. Forward pass: \\( \\mathbf{Z} = \\mathbf{X}\\mathbf{W} + \\mathbf{b} \\), \\( \\hat{\\mathbf{Y}} = \\mathrm{softmax}(\\mathbf{Z}) \\).\n",
    "3. Loss: average cross‑entropy.\n",
    "4. Backprop: gradients for \\( \\mathbf{W}, \\mathbf{b} \\) via the compact **softmax + CE** identity.\n",
    "5. Training loop with gradient descent.\n",
    "6. **Matplotlib animations** for loss and for a single example’s probability vector during training.\n",
    "\n",
    "> **Network layout**: `Input 784 → Linear (784×10) → Softmax → 10 classes`. No hidden layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605bf015",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Math — Forward pass and Loss\n",
    "\n",
    "Let \\(X\\in\\mathbb{R}^{m\\times 784}\\) be a batch of \\(m\\) flattened images, \\(W\\in\\mathbb{R}^{784\\times 10}\\), \\(b\\in\\mathbb{R}^{1\\times 10}\\).  \n",
    "The **logits** (pre‑softmax scores) are\n",
    "$$\n",
    "\\mathbf{Z} = \\mathbf{X}\\mathbf{W} + \\mathbf{b} \\quad\\in\\mathbb{R}^{m\\times 10}.\n",
    "$$\n",
    "Softmax (row‑wise) yields class probabilities:\n",
    "$$\n",
    "\\hat{\\mathbf{Y}}_{i,c} = \\frac{e^{Z_{i,c} - \\max_j Z_{i,j}}}{\\sum_{k=1}^{10} e^{Z_{i,k}-\\max_j Z_{i,j}}}.\n",
    "$$\n",
    "With one‑hot labels \\(Y\\in\\mathbb{R}^{m\\times 10}\\), the average **cross‑entropy** loss is\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{m}\\sum_{i=1}^m \\sum_{c=1}^{10} Y_{i,c}\\,\\log \\hat{Y}_{i,c}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7780013f",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Backprop — compact gradient\n",
    "For softmax + cross‑entropy together,\n",
    "$$\n",
    "\\boxed{\\;\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Z}} = \\hat{\\mathbf{Y}} - \\mathbf{Y}\\;}\n",
    "$$\n",
    "Then, since \\(\\mathbf{Z} = \\mathbf{X}\\mathbf{W} + \\mathbf{b}\\),\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}} = \\frac{1}{m}\\mathbf{X}^\\top(\\hat{\\mathbf{Y}}-\\mathbf{Y}), \n",
    "\\qquad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}} = \\frac{1}{m}\\sum_{i=1}^m (\\hat{\\mathbf{Y}}_i-\\mathbf{Y}_i).\n",
    "$$\n",
    "\n",
    "**Update rule (GD):**\n",
    "$$\n",
    "\\mathbf{W} \\leftarrow \\mathbf{W} - \\eta\\,\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}},\\qquad\n",
    "\\mathbf{b} \\leftarrow \\mathbf{b} - \\eta\\,\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea8c51",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Load MNIST (Keras) and Prepare Data\n",
    "\n",
    "> If your environment is offline, this cell may fail to download MNIST. In that case, replace it with any local MNIST loader or pre‑placed arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12fa8d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_mnist_corrected' from 'utils' (/home/eng/workspace/simple_neural_network_2025b/notebooks/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_mnist_corrected\n\u001b[32m      3\u001b[39m x_train, y_train, x_test, y_test = get_mnist_corrected()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Normalize to [0,1], flatten to (m, 784)\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'get_mnist_corrected' from 'utils' (/home/eng/workspace/simple_neural_network_2025b/notebooks/utils.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "from utils import get_mnist_corrected\n",
    "\n",
    "x_train, y_train, x_test, y_test = get_mnist_corrected()\n",
    "\n",
    "# Normalize to [0,1], flatten to (m, 784)\n",
    "x_train = (x_train.astype(np.float32) / 255.0).reshape(x_train.shape[0], -1)\n",
    "x_test  = (x_test.astype(np.float32)  / 255.0).reshape(x_test.shape[0], -1)\n",
    "\n",
    "num_features = x_train.shape[1]  # 784\n",
    "num_classes = 10\n",
    "\n",
    "# One-hot labels\n",
    "y_train_oh = np.eye(num_classes)[y_train]\n",
    "y_test_oh  = np.eye(num_classes)[y_test]\n",
    "\n",
    "x_train.shape, y_train_oh.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0122ff",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Model pieces (NumPy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c6688",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(z):\n",
    "    # Numerically stable row-wise softmax\n",
    "    z_shift = z - np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z_shift)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(y_true_oh, y_prob):\n",
    "    # Mean CE over batch\n",
    "    return -np.mean(np.sum(y_true_oh * np.log(y_prob + 1e-9), axis=1))\n",
    "\n",
    "def accuracy(y_true, y_prob):\n",
    "    preds = np.argmax(y_prob, axis=1)\n",
    "    return np.mean(preds == y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2828d3",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Training Loop (Full‑batch by default)\n",
    "\n",
    "We record `loss_history`, `acc_history`, and snapshots of **a single example’s** probability vector each epoch to animate later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "epochs = 25           # keep modest for demo/animation speed\n",
    "batch_size = 1024     # use mini-batches for speed/stability\n",
    "\n",
    "m = x_train.shape[0]\n",
    "\n",
    "# Initialize W, b\n",
    "rng = np.random.default_rng(42)\n",
    "W = rng.normal(0, 0.01, size=(num_features, num_classes))\n",
    "b = np.zeros((1, num_classes), dtype=np.float32)\n",
    "\n",
    "# Choose one fixed example to visualize probabilities over time\n",
    "viz_idx = 0\n",
    "viz_x = x_train[viz_idx:viz_idx+1]          # (1,784)\n",
    "viz_label = y_train[viz_idx]\n",
    "\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "viz_prob_history = []   # list of (10,) arrays\n",
    "\n",
    "# Mini-batch indices helper\n",
    "def iterate_minibatches(X, Y, batch_size, shuffle=True):\n",
    "    n = X.shape[0]\n",
    "    indices = np.arange(n)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = min(start + batch_size, n)\n",
    "        batch_idx = indices[start:end]\n",
    "        yield X[batch_idx], Y[batch_idx]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training epoch (mini-batch SGD)\n",
    "    for Xb, Yb in iterate_minibatches(x_train, y_train_oh, batch_size, shuffle=True):\n",
    "        scores = Xb @ W + b                 # (B,10)\n",
    "        probs  = softmax(scores)            # (B,10)\n",
    "\n",
    "        # Gradients\n",
    "        d_scores = probs - Yb               # (B,10)\n",
    "        dW = (Xb.T @ d_scores) / Xb.shape[0]\n",
    "        db = np.sum(d_scores, axis=0, keepdims=True) / Xb.shape[0]\n",
    "\n",
    "        # Update\n",
    "        W -= learning_rate * dW\n",
    "        b -= learning_rate * db\n",
    "\n",
    "    # End-of-epoch metrics (full train set for simplicity)\n",
    "    train_scores = x_train @ W + b\n",
    "    train_probs  = softmax(train_scores)\n",
    "    L = cross_entropy(y_train_oh, train_probs)\n",
    "    A = accuracy(y_train, train_probs)\n",
    "    loss_history.append(L)\n",
    "    acc_history.append(A)\n",
    "\n",
    "    # Store viz probabilities for the fixed example\n",
    "    viz_prob = softmax(viz_x @ W + b)[0]  # (10,)\n",
    "    viz_prob_history.append(viz_prob.copy())\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}/{epochs}  |  loss={L:.4f}  acc={A*100:.2f}%\")\n",
    "\n",
    "# Final test accuracy (optional)\n",
    "test_probs = softmax(x_test @ W + b)\n",
    "test_acc = accuracy(y_test, test_probs)\n",
    "print(f\"Test accuracy: {test_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b43166",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Animation — Loss over Epochs\n",
    "\n",
    "This animates the training loss as it evolves per epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e289e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a loss animation\n",
    "fig1, ax1 = plt.subplots(figsize=(6,4))\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Training Loss Over Epochs\")\n",
    "line1, = ax1.plot([], [])  # no explicit colors\n",
    "\n",
    "ax1.set_xlim(1, len(loss_history))\n",
    "ax1.set_ylim(min(loss_history)*0.95, max(loss_history)*1.05)\n",
    "\n",
    "def init_loss():\n",
    "    line1.set_data([], [])\n",
    "    return (line1,)\n",
    "\n",
    "def update_loss(frame):\n",
    "    xdata = np.arange(1, frame+2)\n",
    "    ydata = loss_history[:frame+1]\n",
    "    line1.set_data(xdata, ydata)\n",
    "    return (line1,)\n",
    "\n",
    "ani1 = animation.FuncAnimation(fig1, update_loss, frames=len(loss_history),\n",
    "                               init_func=init_loss, interval=300, blit=True)\n",
    "HTML(ani1.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e7d582",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Animation — Probability Vector for One Example\n",
    "\n",
    "We track the predicted probability distribution over the 10 digits for a **fixed** training image across epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2ea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prob_array = np.stack(viz_prob_history, axis=0)  # (epochs, 10)\n",
    "x_classes = np.arange(10)\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(6,4))\n",
    "ax2.set_xlabel(\"Class\")\n",
    "ax2.set_ylabel(\"Probability\")\n",
    "ax2.set_title(f\"Predicted Probabilities for Sample idx={0} (true={viz_label})\")\n",
    "bar_container = ax2.bar(x_classes, prob_array[0])  # default colors\n",
    "\n",
    "ax2.set_ylim(0.0, 1.0)\n",
    "ax2.set_xticks(x_classes)\n",
    "\n",
    "def update_bars(frame):\n",
    "    probs = prob_array[frame]\n",
    "    for rect, h in zip(bar_container, probs):\n",
    "        rect.set_height(float(h))\n",
    "    return bar_container.patches\n",
    "\n",
    "ani2 = animation.FuncAnimation(fig2, update_bars, frames=prob_array.shape[0],\n",
    "                               interval=300, blit=False)\n",
    "HTML(ani2.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d779856",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Inference Helper (Try a few test digits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09dc011",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_proba(X):\n",
    "    return softmax(X @ W + b)\n",
    "\n",
    "def predict(X):\n",
    "    return np.argmax(predict_proba(X), axis=1)\n",
    "\n",
    "preds = predict(x_test[:10])\n",
    "print(\"Predictions for first 10 test digits:\", preds)\n",
    "print(\"Ground truth:\", y_test[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db68fba",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Summary\n",
    "\n",
    "- **Architecture**: `784 → Linear (W,b) → Softmax → 10`.\n",
    "- **Forward**: \\(Z = XW + b\\), \\(\\hat{Y}=\\mathrm{softmax}(Z)\\).\n",
    "- **Loss**: mean cross‑entropy.\n",
    "- **Backprop**: \\(\\\\frac{\\\\partial L}{\\\\partial Z} = \\\\hat{Y}-Y\\),  \n",
    "  \\(\\\\frac{\\\\partial L}{\\\\partial W} = \\\\frac{1}{m}X^\\\\top(\\\\hat{Y}-Y)\\),  \n",
    "  \\(\\\\frac{\\\\partial L}{\\\\partial b} = \\\\frac{1}{m}\\\\sum_i(\\\\hat{Y}_i-Y_i)\\\\).\n",
    "- **Training**: gradient descent on \\(W,b\\).\n",
    "- **Animations**: loss over epochs; probability bars for one example.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
